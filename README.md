# DeepSeek V3 å®Œæ•´è®­ç»ƒæŒ‡å—

> æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªæ•™è‚²æ€§çš„ DeepSeek V3 å®ç°ï¼Œæ¶µç›– **Pretrain â†’ SFT â†’ RL** å®Œæ•´è®­ç»ƒæµç¨‹ã€‚

---

## ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#1-é¡¹ç›®æ¦‚è¿°)
2. [æ¶æ„è®¾è®¡](#2-æ¶æ„è®¾è®¡)
   - [Multi-head Latent Attention (MLA)](#21-multi-head-latent-attention-mla)
   - [DeepSeekMoE (æ··åˆä¸“å®¶)](#22-deepseekmoemixtrue-of-experts)
   - [Multi-Token Prediction (MTP)](#23-multi-token-prediction-mtp)
3. [é¡¹ç›®ç»“æ„](#3-é¡¹ç›®ç»“æ„)
4. [æ•°æ®é›†è¯´æ˜](#4-æ•°æ®é›†è¯´æ˜)
5. [Tokenizer å¤„ç†](#5-tokenizer-å¤„ç†)
6. [è®­ç»ƒæµç¨‹](#6-è®­ç»ƒæµç¨‹)
   - [Pretrain é¢„è®­ç»ƒ](#61-pretrain-é¢„è®­ç»ƒ)
   - [SFT æœ‰ç›‘ç£å¾®è°ƒ](#62-sft-æœ‰ç›‘ç£å¾®è°ƒ)
   - [RL å¼ºåŒ–å­¦ä¹ ](#63-rl-å¼ºåŒ–å­¦ä¹ )
7. [RL å¼ºåŒ–å­¦ä¹ è¯¦è§£](#7-rl-å¼ºåŒ–å­¦ä¹ è¯¦è§£)
   - [GRPO ç®—æ³•åŸç†](#71-grpo-group-relative-policy-optimization)
   - [PPO ç®—æ³•åŸç†](#72-ppo-proximal-policy-optimization)
   - [DPO ç®—æ³•åŸç†](#73-dpo-direct-preference-optimization)
   - [Loss å‡½æ•°ä¸æ³¨æ„äº‹é¡¹](#74-loss-å‡½æ•°ä¸æ³¨æ„äº‹é¡¹)
8. [å¿«é€Ÿå¼€å§‹](#8-å¿«é€Ÿå¼€å§‹)
9. [é…ç½®è¯´æ˜](#9-é…ç½®è¯´æ˜)
10. [ç›‘æ§ä¸å¯è§†åŒ–](#10-ç›‘æ§ä¸å¯è§†åŒ–)

---

## 1. é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®å®ç°äº† DeepSeek V3 çš„æ ¸å¿ƒæ¶æ„å’Œå®Œæ•´è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š

| é˜¶æ®µ | æè¿° | æ•°æ®é›† | ç›®æ ‡ |
|------|------|--------|------|
| **Pretrain** | è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒ | WikiText-2 / OpenWebText | å­¦ä¹ è¯­è¨€çŸ¥è¯† |
| **SFT** | æœ‰ç›‘ç£å¾®è°ƒ | Alpaca | å­¦ä¹ æŒ‡ä»¤è·Ÿéš |
| **RL** | å¼ºåŒ–å­¦ä¹ å¯¹é½ | HH-RLHF | ä¸äººç±»åå¥½å¯¹é½ |

### æ ¸å¿ƒåˆ›æ–°

1. **MLA (Multi-head Latent Attention)**: ä½ç§© KV å‹ç¼©ï¼Œå‡å°‘æ¨ç†æ—¶å†…å­˜å ç”¨
2. **DeepSeekMoE**: å…±äº«ä¸“å®¶ + è·¯ç”±ä¸“å®¶çš„æ··åˆä¸“å®¶æ¶æ„
3. **MTP (Multi-Token Prediction)**: å¤š token é¢„æµ‹ä½œä¸ºè¾…åŠ©è®­ç»ƒç›®æ ‡

---

## 2. æ¶æ„è®¾è®¡

### 2.1 Multi-head Latent Attention (MLA)

MLA æ˜¯ DeepSeek V3 çš„æ ¸å¿ƒæ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡ä½ç§©å‹ç¼© KV æ¥å‡å°‘å†…å­˜å ç”¨ã€‚

#### åŸç†

ä¼ ç»Ÿ Attention çš„ KV ç¼“å­˜å¤§å°ä¸º `O(H Ã— d_h)`ï¼ŒMLA å°†å…¶å‹ç¼©åˆ° `O(d_c)`ã€‚

```
Input: x âˆˆ R^(B Ã— L Ã— D)

# KV å‹ç¼© (æ ¸å¿ƒåˆ›æ–°)
c_kv = W_down(x)           # (B, L, d_c)     - å‹ç¼©åˆ°ä½ç»´
K = W_k_up(c_kv)           # (B, L, H, d_h)  - æ‰©å±•ä¸º Key
V = W_v_up(c_kv)           # (B, L, H, d_h^v)- æ‰©å±•ä¸º Value

# Query ä½¿ç”¨ç‹¬ç«‹å‹ç¼©
c_q = W_q_down(x)          # (B, L, d_c')
Q = W_q_up(c_q)            # (B, L, H, d_h)

# Decoupled RoPE (è§£è€¦ä½ç½®ç¼–ç )
Q_nope, Q_rope = split(Q)  # åˆ†ç¦»ä½ç½®ç›¸å…³å’Œä½ç½®æ— å…³éƒ¨åˆ†
K_nope, K_rope = split(K)

# ä»…å¯¹ rope éƒ¨åˆ†åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
Q_rope, K_rope = apply_rope(Q_rope, K_rope)

# é‡æ–°ç»„åˆ
Q = concat(Q_nope, Q_rope)
K = concat(K_nope, K_rope)

# æ ‡å‡† Attention
Output = softmax(QK^T / âˆšd_h) Â· V
```

#### å…³é”®å‚æ•°

| å‚æ•° | å«ä¹‰ | é»˜è®¤å€¼ |
|------|------|--------|
| `kv_lora_rank` | KV å‹ç¼©ç»´åº¦ d_c | 64 |
| `q_lora_rank` | Q å‹ç¼©ç»´åº¦ d_c' | 96 |
| `qk_nope_head_dim` | é RoPE å¤´ç»´åº¦ | 32 |
| `qk_rope_head_dim` | RoPE å¤´ç»´åº¦ | 32 |
| `v_head_dim` | Value å¤´ç»´åº¦ | 64 |

#### ä»£ç ä½ç½®

- å®ç°: [attention.py](attention.py) - `MultiHeadLatentAttention` ç±»

---

### 2.2 DeepSeekMoEï¼ˆMixture of Expertsï¼‰

DeepSeekMoE ç»“åˆäº†å…±äº«ä¸“å®¶å’Œè·¯ç”±ä¸“å®¶ï¼Œæ—¢ä¿è¯é€šç”¨çŸ¥è¯†åˆæä¾›ä¸“ä¸šèƒ½åŠ›ã€‚

#### æ¶æ„

```
Input: x âˆˆ R^(B Ã— L Ã— D)

# 1. å…±äº«ä¸“å®¶ (Shared Experts) - å§‹ç»ˆæ¿€æ´»
shared_out = Î£ expert_s(x) / n_shared

# 2. è·¯ç”±ä¸“å®¶ (Routed Experts) - Top-K é€‰æ‹©
router_probs = softmax(gate(x))           # (B, L, N) è·¯ç”±æ¦‚ç‡
top_k_probs, top_k_idx = topk(router_probs, K)  # é€‰æ‹© Top-K ä¸“å®¶
routed_out = Î£ (prob_i Ã— expert_i(x))     # åŠ æƒè¾“å‡º

# 3. æœ€ç»ˆè¾“å‡º
output = shared_out + routed_scaling_factor Ã— routed_out
```

#### è´Ÿè½½å‡è¡¡æŸå¤±

ä¸ºäº†é˜²æ­¢ä¸“å®¶ä½¿ç”¨ä¸å‡è¡¡ï¼Œå¼•å…¥è¾…åŠ©æŸå¤±ï¼š

```
L_aux = Î± Ã— N Ã— Î£(f_i Ã— P_i)

å…¶ä¸­:
- f_i: ä¸“å®¶ i æ¥æ”¶çš„ token æ¯”ä¾‹
- P_i: ä¸“å®¶ i çš„å¹³å‡è·¯ç”±æ¦‚ç‡
- Î±: æŸå¤±ç³»æ•° (é»˜è®¤ 0.001)
- N: ä¸“å®¶æ€»æ•°
```

#### å…³é”®å‚æ•°

| å‚æ•° | å«ä¹‰ | é»˜è®¤å€¼ |
|------|------|--------|
| `num_experts` | è·¯ç”±ä¸“å®¶æ€»æ•° N | 16 |
| `num_experts_per_tok` | æ¯ token æ¿€æ´»ä¸“å®¶æ•° K | 2 |
| `num_shared_experts` | å…±äº«ä¸“å®¶æ•° | 2 |
| `expert_hidden_size` | ä¸“å®¶ FFN éšè—ç»´åº¦ | 768 |
| `aux_loss_alpha` | è¾…åŠ©æŸå¤±ç³»æ•° | 0.001 |

#### ä»£ç ä½ç½®

- å®ç°: [model.py](model.py) - `DeepSeekMoE`, `MoEGate`, `Expert` ç±»

---

### 2.3 Multi-Token Prediction (MTP)

MTP åŒæ—¶é¢„æµ‹å¤šä¸ªæœªæ¥ tokenï¼Œä½œä¸ºè¾…åŠ©è®­ç»ƒç›®æ ‡ï¼ŒåŒæ—¶æ”¯æŒæ¨ç†æ—¶çš„æŠ•æœºè§£ç ã€‚

#### åŸç†

```
Input: hidden_states âˆˆ R^(B Ã— L Ã— D)

# å¯¹æ¯ä¸ªé¢„æµ‹æ·±åº¦ d âˆˆ [1, D_predict]
for d in range(1, num_predict_tokens + 1):
    # ç‹¬ç«‹çš„æŠ•å½±å±‚
    h_d = projection_d(hidden_states)
    h_d = layer_norm_d(h_d)
    logits_d = output_head_d(h_d)  # é¢„æµ‹ä½ç½® i+d å¤„çš„ token

# è®­ç»ƒæ—¶è®¡ç®— MTP Loss
mtp_loss = Î£ CE(logits_d[:, :-d-1], labels[:, d+1:])
total_loss = lm_loss + mtp_weight Ã— mtp_loss
```

#### å…³é”®å‚æ•°

| å‚æ•° | å«ä¹‰ | é»˜è®¤å€¼ |
|------|------|--------|
| `num_predict_tokens` | é¢å¤–é¢„æµ‹çš„ token æ•° | 2 |
| `mtp_loss_weight` | MTP æŸå¤±æƒé‡ | 0.3 |

#### ä»£ç ä½ç½®

- å®ç°: [model.py](model.py) - `MTPHead` ç±»

---

## 3. é¡¹ç›®ç»“æ„

```
deepseek_v3/
â”œâ”€â”€ config.py               # é…ç½®ç®¡ç† (æ‰€æœ‰é…ç½®ç±»å®šä¹‰)
â”œâ”€â”€ config_default.yaml     # é»˜è®¤é…ç½® (å°æ•°æ®é›†)
â”œâ”€â”€ config_large.yaml       # å¤§è§„æ¨¡è®­ç»ƒé…ç½®
â”œâ”€â”€ attention.py            # MLA æ³¨æ„åŠ›å®ç°
â”œâ”€â”€ model.py                # DeepSeek V3 æ¨¡å‹ä¸»ä½“
â”œâ”€â”€ dataset.py              # æ•°æ®é›†å¤„ç† (Pretrain/SFT/RL)
â”œâ”€â”€ rl_dataset.py           # RL ä¸“ç”¨æ•°æ®é›†
â”œâ”€â”€ trainer.py              # è®­ç»ƒå™¨ (Pretrain/SFT/GRPO)
â”œâ”€â”€ rl_trainer_base.py      # RL è®­ç»ƒåŸºç±»
â”œâ”€â”€ rl_trainer_algorithms.py # RL ç®—æ³•å®ç° (GRPO/PPO)
â”œâ”€â”€ train.py                # è®­ç»ƒå…¥å£è„šæœ¬
â”œâ”€â”€ rl_train.py             # RL è®­ç»ƒå…¥å£
â”œâ”€â”€ inference.py            # æ¨ç†å’Œç”Ÿæˆ
â”œâ”€â”€ logger.py               # æ—¥å¿—æ¨¡å— (å½©è‰²è¾“å‡ºã€å¤šçº§åˆ«æ—¥å¿—)
â”œâ”€â”€ run.sh                  # ä¾¿æ·è¿è¡Œè„šæœ¬
â”œâ”€â”€ run_pretrain.sh         # é¢„è®­ç»ƒä¸“ç”¨è„šæœ¬
â”œâ”€â”€ test_all.py             # æµ‹è¯•å¥—ä»¶
â”œâ”€â”€ test_rl.py              # RL æµ‹è¯•
â”œâ”€â”€ requirements.txt        # Python ä¾èµ–
â””â”€â”€ README.md               # é¡¹ç›®è¯´æ˜
```

### 3.1 æ—¥å¿—æ¨¡å— (logger.py)

é¡¹ç›®æä¾›ç»Ÿä¸€çš„æ—¥å¿—ç®¡ç†åŠŸèƒ½ï¼Œæ”¯æŒå½©è‰²è¾“å‡ºå’Œä¸åŒæ—¥å¿—çº§åˆ«ã€‚

#### æ—¥å¿—çº§åˆ«é¢œè‰²

| çº§åˆ« | é¢œè‰² | ç¬¦å· |
|------|------|------|
| DEBUG | é’è‰² (Cyan) | ğŸ” |
| INFO | ç»¿è‰² (Green) | â„¹ï¸ |
| WARNING | é»„è‰² (Yellow) | âš ï¸ |
| ERROR | çº¢è‰² (Red) | âŒ |
| CRITICAL | ç²—ä½“çº¢è‰² | ğŸ”¥ |

#### æ—¥å¿—æ ¼å¼

```
[æ—¶é—´] [PID:TID] [ç¬¦å·] [çº§åˆ«] [æ–‡ä»¶:è¡Œå·] æ¶ˆæ¯
```

ç¤ºä¾‹:
```
2025-12-27 10:30:00 [12345:67890] â„¹ï¸  [  INFO  ] [train.py:100] Training started
```

#### ä½¿ç”¨æ–¹æ³•

```python
from logger import get_logger, set_log_level
import logging

# è·å– logger
logger = get_logger(__name__)

# ä½¿ç”¨ä¸åŒçº§åˆ«çš„æ—¥å¿—
logger.debug("Detailed debugging info")
logger.info("General information")
logger.warning("Potential issue")
logger.error("Error occurred")

# è®¾ç½®å…¨å±€æ—¥å¿—çº§åˆ«
set_log_level(logging.DEBUG)

# æ·»åŠ æ–‡ä»¶æ—¥å¿—
from logger import setup_file_logging
setup_file_logging("logs/training.log")
```

---

## 4. æ•°æ®é›†è¯´æ˜

### 4.1 Pretrain æ•°æ®é›†

| æ•°æ®é›† | è§„æ¨¡ | å‚æ•° | ç”¨é€” |
|--------|------|------|------|
| **WikiText-2** | ~13MB | `--dataset_scale small` | å¿«é€Ÿæµ‹è¯•/å®éªŒ |
| **OpenWebText** | ~40GB | `--dataset_scale large` | æ­£å¼è®­ç»ƒ |

#### WikiText-2 æ ¼å¼
åŸå§‹æ–‡æœ¬ï¼Œæ¯è¡Œæ˜¯ä¸€æ®µæ–‡ç« ï¼š
```text
= Valkyria Chronicles III =
SenjÅ no Valkyria 3 : Unrecorded Chronicles ( Japanese : æˆ¦å ´ã®...
```

#### OpenWebText æ ¼å¼
Reddit å¤–é“¾æ–‡ç« çš„æ–‡æœ¬å†…å®¹ï¼š
```python
{
    "text": "The full article content..."
}
```

### 4.2 SFT æ•°æ®é›†

| æ•°æ®é›† | è§„æ¨¡ | æ ¼å¼ |
|--------|------|------|
| **Alpaca** | ~52K æ ·æœ¬ | instruction-input-output |

#### Alpaca æ ¼å¼
```json
{
    "instruction": "Give three tips for staying healthy.",
    "input": "",
    "output": "1. Eat a balanced diet...\n2. Exercise regularly...\n3. Get enough sleep..."
}
```

#### æ ¼å¼åŒ–æ¨¡æ¿
```
### Instruction:
{instruction}

### Input:
{input}

### Response:
{output}
```

### 4.3 RL æ•°æ®é›†

| æ•°æ®é›† | è§„æ¨¡ | æ ¼å¼ |
|--------|------|------|
| **HH-RLHF** | ~170K | chosen/rejected pairs |

#### HH-RLHF æ ¼å¼
```json
{
    "chosen": "Human: What is...\n\nAssistant: The answer is...",
    "rejected": "Human: What is...\n\nAssistant: I don't know..."
}
```

---

## 5. Tokenizer å¤„ç†

æœ¬é¡¹ç›®ä½¿ç”¨ GPT-2 Tokenizerï¼ˆå¯é…ç½®å…¶ä»– HuggingFace tokenizerï¼‰ã€‚

### 5.1 åŠ è½½ Tokenizer

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")

# è®¾ç½® padding token (GPT-2 é»˜è®¤æ²¡æœ‰)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id
```

### 5.2 Pretrain æ•°æ®å¤„ç†

```python
# 1. å°†æ‰€æœ‰æ–‡æœ¬æ‹¼æ¥
all_text = " ".join(texts)

# 2. Tokenize
tokens = tokenizer.encode(all_text, add_special_tokens=False)

# 3. åˆ‡åˆ†æˆå›ºå®šé•¿åº¦çš„åºåˆ—
for i in range(0, len(tokens) - max_seq_length, max_seq_length):
    chunk = tokens[i:i + max_seq_length]
    examples.append({
        'input_ids': torch.tensor(chunk),
        'attention_mask': torch.ones(len(chunk)),
        'labels': torch.tensor(chunk),  # è‡ªå›å½’ï¼Œlabels = input_ids
    })
```

### 5.3 SFT æ•°æ®å¤„ç†

```python
# 1. æ ¼å¼åŒ– prompt å’Œå®Œæ•´æ–‡æœ¬
prompt = f"### Instruction:\n{instruction}\n\n### Response:\n"
full_text = prompt + output

# 2. Tokenize
prompt_ids = tokenizer.encode(prompt)
full_ids = tokenizer.encode(full_text)

# 3. åˆ›å»º labels (prompt éƒ¨åˆ†ä¸º -100ï¼Œä¸è®¡ç®— loss)
labels = [-100] * len(prompt_ids) + full_ids[len(prompt_ids):]
```

### 5.4 å…³é”®é…ç½®

```yaml
data:
  tokenizer_name: "gpt2"           # å¯æ”¹ä¸ºå…¶ä»– tokenizer
  pretrain_max_seq_length: 512     # é¢„è®­ç»ƒåºåˆ—é•¿åº¦
  sft_max_seq_length: 512          # SFT åºåˆ—é•¿åº¦
  rl_max_seq_length: 256           # RL åºåˆ—é•¿åº¦
```

---

## 6. è®­ç»ƒæµç¨‹

### 6.1 Pretrain é¢„è®­ç»ƒ

#### ç›®æ ‡
å­¦ä¹ è¯­è¨€æ¨¡å‹çš„åŸºç¡€èƒ½åŠ›ï¼šè¯­æ³•ã€çŸ¥è¯†ã€æ¨ç†ã€‚

#### Loss å‡½æ•°
```python
# Next-Token Prediction Loss
loss = CrossEntropyLoss(logits[:, :-1], labels[:, 1:])

# + MTP Loss (if enabled)
for d in range(1, num_predict_tokens + 1):
    mtp_loss += CrossEntropyLoss(mtp_logits_d[:, :-d-1], labels[:, d+1:])
loss += mtp_weight * mtp_loss / num_predict_tokens

# + MoE Auxiliary Loss (if enabled)
loss += aux_loss
```

#### è¿è¡Œå‘½ä»¤

```bash
# å°æ•°æ®é›†å¿«é€Ÿæµ‹è¯• (WikiText-2, ~13MB)
python train.py --mode pretrain --dataset_scale small --test

# å°æ•°æ®é›†å®Œæ•´è®­ç»ƒ
python train.py --mode pretrain --dataset_scale small

# å¤§æ•°æ®é›†è®­ç»ƒ (OpenWebText, ~10GB)
python train.py --mode pretrain --dataset_scale large

# ä½¿ç”¨ run.sh è„šæœ¬
./run.sh pretrain          # å°æ•°æ®é›†
./run.sh pretrain-large    # å¤§æ•°æ®é›†
./run.sh pretrain-test     # å¿«é€Ÿæµ‹è¯•
```

#### å…³é”®å‚æ•°

```yaml
pretraining:
  batch_size: 16
  learning_rate: 3e-4
  max_steps: 5000
  warmup_steps: 200
  gradient_accumulation_steps: 2
  max_grad_norm: 1.0
```

---

### 6.2 SFT æœ‰ç›‘ç£å¾®è°ƒ

#### ç›®æ ‡
å­¦ä¹ éµå¾ªæŒ‡ä»¤ã€ç”Ÿæˆæœ‰å¸®åŠ©çš„å›å¤ã€‚

#### Loss å‡½æ•°
```python
# åªåœ¨ response éƒ¨åˆ†è®¡ç®— loss
# labels ä¸­ prompt éƒ¨åˆ†è®¾ä¸º -100
loss = CrossEntropyLoss(logits, labels, ignore_index=-100)
```

#### è¿è¡Œå‘½ä»¤

```bash
# ä»é¢„è®­ç»ƒæ£€æŸ¥ç‚¹å¼€å§‹ SFT
python train.py --mode sft --checkpoint checkpoints/pretrain/best.pt

# ä½¿ç”¨ run.sh
./run.sh sft checkpoints/pretrain/best.pt
./run.sh sft-test  # å¿«é€Ÿæµ‹è¯•
```

#### å…³é”®å‚æ•°

```yaml
sft:
  batch_size: 8
  learning_rate: 2e-5      # æ¯”é¢„è®­ç»ƒå°
  max_steps: 2000
  warmup_ratio: 0.03
  weight_decay: 0.0        # SFT é€šå¸¸ä¸ç”¨ weight decay
```

---

### 6.3 RL å¼ºåŒ–å­¦ä¹ 

#### ç›®æ ‡
å°†æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ï¼Œç”Ÿæˆæ›´æœ‰å¸®åŠ©ã€æ›´å®‰å…¨çš„å›å¤ã€‚

#### æ”¯æŒçš„ç®—æ³•

| ç®—æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **GRPO** | Online | DeepSeek é£æ ¼ï¼Œç»„å†…ç›¸å¯¹ä¼˜åŠ¿ |
| **PPO** | Online | ç»å…¸ RLHFï¼Œéœ€è¦ value function |
| **DPO** | Offline | ç›´æ¥ä¼˜åŒ–åå¥½ï¼Œæ— éœ€ reward model |

#### è¿è¡Œå‘½ä»¤

```bash
# GRPO (é»˜è®¤)
python train.py --mode rl --checkpoint checkpoints/sft/best.pt

# æŒ‡å®šç®—æ³•
python rl_train.py --algorithm grpo --checkpoint checkpoints/sft/best.pt
python rl_train.py --algorithm ppo --checkpoint checkpoints/sft/best.pt
python rl_train.py --algorithm dpo --checkpoint checkpoints/sft/best.pt

# ä½¿ç”¨ run.sh
./run.sh rl checkpoints/sft/best.pt
./run.sh rl-test
```

---

## 7. RL å¼ºåŒ–å­¦ä¹ è¯¦è§£

### 7.1 GRPO (Group Relative Policy Optimization)

GRPO æ˜¯ DeepSeek æå‡ºçš„ RL ç®—æ³•ï¼Œæ— éœ€å­¦ä¹  reward modelï¼Œä½¿ç”¨ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ã€‚

#### ç®—æ³•æµç¨‹

```
For each prompt x:
    1. ç”Ÿæˆ G ä¸ªå“åº” {y_1, y_2, ..., y_G}
    2. è®¡ç®—æ¯ä¸ªå“åº”çš„ reward r_i = R(x, y_i)
    3. è®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿:
       A_i = (r_i - mean(r)) / (std(r) + Îµ)
    4. è®¡ç®— policy gradient loss:
       L_PG = -E[A_i Ã— log Ï€(y_i|x)]
    5. è®¡ç®— KL æƒ©ç½š:
       L_KL = Î² Ã— KL(Ï€ || Ï€_ref)
    6. æ€»æŸå¤±:
       L = L_PG + L_KL
```

#### æ ¸å¿ƒä»£ç 

```python
# ç»„å†…ç›¸å¯¹ä¼˜åŠ¿å½’ä¸€åŒ–
rewards_t = torch.tensor(rewards)
mean_r = rewards_t.mean()
std_r = rewards_t.std() + 1e-8
advantages = (rewards_t - mean_r) / std_r

# Policy Gradient Loss
for adv, log_prob in zip(advantages, log_probs):
    pg_loss += -adv * log_prob.mean()

# KL Penalty
kl = (policy_logps - ref_logps).mean()
loss = pg_loss + kl_coef * kl
```

#### å…³é”®å‚æ•°

| å‚æ•° | å«ä¹‰ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `group_size` | æ¯ä¸ª prompt ç”Ÿæˆçš„å“åº”æ•° | 4 | è¶Šå¤§æ–¹å·®ä¼°è®¡è¶Šå‡†ï¼Œä½†è®¡ç®—é‡å¤§ |
| `kl_coef` | KL æƒ©ç½šç³»æ•° Î² | 0.1 | é˜²æ­¢åç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œ |
| `temperature` | é‡‡æ ·æ¸©åº¦ | 0.7 | æ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§ |

---

### 7.2 PPO (Proximal Policy Optimization)

PPO æ˜¯ç»å…¸çš„ RLHF ç®—æ³•ï¼Œä½¿ç”¨ value function ä¼°è®¡ä¼˜åŠ¿ã€‚

#### ç®—æ³•æµç¨‹

```
1. Rollout: ç”Ÿæˆå“åº”ï¼Œè®¡ç®— reward
2. è®¡ç®— GAE (Generalized Advantage Estimation):
   Î´_t = r_t + Î³ V(s_{t+1}) - V(s_t)
   A_t = Î£ (Î³Î»)^k Î´_{t+k}
3. PPO Update (å¤šä¸ª epoch):
   a. è®¡ç®— probability ratio: Ï = Ï€(a|s) / Ï€_old(a|s)
   b. Clipped surrogate objective:
      L_clip = min(Ï A, clip(Ï, 1-Îµ, 1+Îµ) A)
   c. Value function loss:
      L_VF = MSE(V(s), R_t)
   d. Entropy bonus:
      H = -Î£ Ï€ log Ï€
   e. Total loss:
      L = -L_clip + c1 Ã— L_VF - c2 Ã— H
```

#### å…³é”®å‚æ•°

| å‚æ•° | å«ä¹‰ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `clip_range` | PPO è£å‰ªèŒƒå›´ Îµ | 0.2 | é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ |
| `value_coef` | Value loss ç³»æ•° c1 | 0.5 | |
| `entropy_coef` | Entropy bonus ç³»æ•° c2 | 0.01 | é¼“åŠ±æ¢ç´¢ |
| `gae_lambda` | GAE Î» å‚æ•° | 0.95 | æ–¹å·®-åå·®æƒè¡¡ |
| `ppo_epochs` | æ¯æ‰¹æ•°æ®çš„ PPO æ›´æ–°æ¬¡æ•° | 4 | |
| `target_kl` | KL æ—©åœé˜ˆå€¼ | 0.02 | è¶…è¿‡åˆ™åœæ­¢æ›´æ–° |

---

### 7.3 DPO (Direct Preference Optimization)

DPO ç›´æ¥ä»åå¥½æ•°æ®å­¦ä¹ ï¼Œæ— éœ€æ˜¾å¼ reward modelã€‚

#### ç®—æ³•åŸç†

```
ç»™å®šåå¥½æ•°æ® (x, y_w, y_l)ï¼Œå…¶ä¸­ y_w æ˜¯äººç±»åå¥½çš„å“åº”ï¼Œy_l æ˜¯ä¸åå¥½çš„å“åº”

DPO Loss:
L_DPO = -E[log Ïƒ(Î² Ã— (log Ï€(y_w|x)/Ï€_ref(y_w|x) - log Ï€(y_l|x)/Ï€_ref(y_l|x)))]

ç®€åŒ–ä¸º:
L_DPO = -E[log Ïƒ(Î² Ã— (r_w - r_l))]

å…¶ä¸­:
r = log Ï€(y|x) - log Ï€_ref(y|x)  # éšå¼ reward
```

#### æ ¸å¿ƒä»£ç 

```python
# è®¡ç®— policy å’Œ reference çš„ log probabilities
policy_logps_w = compute_log_probs(model, y_w)
policy_logps_l = compute_log_probs(model, y_l)
ref_logps_w = compute_log_probs(ref_model, y_w)
ref_logps_l = compute_log_probs(ref_model, y_l)

# è®¡ç®— reward margin
logits_w = policy_logps_w - ref_logps_w
logits_l = policy_logps_l - ref_logps_l
logits_diff = logits_w - logits_l

# DPO Loss
loss = -F.logsigmoid(beta * logits_diff).mean()
```

#### å…³é”®å‚æ•°

| å‚æ•° | å«ä¹‰ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `dpo_beta` | æ¸©åº¦å‚æ•° Î² | 0.1 | è¶Šå¤§ç­–ç•¥å˜åŒ–è¶Šæ¿€è¿› |
| `dpo_label_smoothing` | æ ‡ç­¾å¹³æ»‘ | 0.0 | å¢åŠ é²æ£’æ€§ |

---

### 7.4 Loss å‡½æ•°ä¸æ³¨æ„äº‹é¡¹

#### RL è®­ç»ƒçš„ä¸»è¦ Loss ç»„ä»¶

| Loss | å…¬å¼ | ä½œç”¨ |
|------|------|------|
| **Policy Gradient** | `-A Ã— log Ï€(y|x)` | æé«˜é«˜ reward å“åº”çš„æ¦‚ç‡ |
| **KL Penalty** | `Î² Ã— KL(Ï€ || Ï€_ref)` | é˜²æ­¢åç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œ |
| **Value Loss** | `MSE(V, R)` | å‡†ç¡®ä¼°è®¡çŠ¶æ€ä»·å€¼ |
| **Entropy Bonus** | `-H(Ï€)` | é¼“åŠ±æ¢ç´¢ |

#### è®­ç»ƒæ³¨æ„äº‹é¡¹

1. **å­¦ä¹ ç‡**ï¼šRL é˜¶æ®µä½¿ç”¨éå¸¸å°çš„å­¦ä¹ ç‡ï¼ˆ~5e-7ï¼‰ï¼Œé˜²æ­¢æ¨¡å‹èƒ½åŠ›é€€åŒ–

2. **KL æ§åˆ¶**ï¼š
   - ç›‘æ§ KL æ•£åº¦ï¼Œè¿‡å¤§è¯´æ˜ç­–ç•¥å˜åŒ–å¤ªå¿«
   - ä½¿ç”¨ target_kl æ—©åœæœºåˆ¶
   - é€‚å½“è°ƒæ•´ kl_coef

3. **Reward è®¾è®¡**ï¼š
   - æœ¬é¡¹ç›®ä½¿ç”¨è§„åˆ™ rewardï¼ˆé•¿åº¦ã€è¿è´¯æ€§ã€é‡å¤æƒ©ç½šç­‰ï¼‰
   - ç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ learned reward model

4. **Reward Hacking**ï¼š
   - æ¨¡å‹å¯èƒ½æ‰¾åˆ° reward çš„æ¼æ´
   - ä½¿ç”¨å¤šæ ·åŒ–çš„ reward ä¿¡å·
   - ä¿æŒ KL çº¦æŸ

5. **è®­ç»ƒç¨³å®šæ€§**ï¼š
   - ä½¿ç”¨æ¢¯åº¦è£å‰ª `max_grad_norm: 1.0`
   - ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯å¹³æ»‘æ›´æ–°
   - ç›‘æ§ reward å’Œ loss æ›²çº¿

6. **Reference Model**ï¼š
   - ä¿æŒå†»ç»“ï¼Œä¸æ›´æ–°å‚æ•°
   - ç”¨äºè®¡ç®— KL æ•£åº¦
   - é˜²æ­¢æ¨¡å‹é€€åŒ–

---

## 8. å¿«é€Ÿå¼€å§‹

### 8.1 å®‰è£…ä¾èµ–

```bash
cd learn/deepseek_v3
pip install -r requirements.txt
chmod +x run.sh
```

### 8.2 è¿è¡Œæµ‹è¯•

```bash
# æµ‹è¯•æ¨¡å‹å’Œè®­ç»ƒæµç¨‹
./run.sh test-quick

# å®Œæ•´æµ‹è¯•
./run.sh test
```

### 8.3 å®Œæ•´è®­ç»ƒæµç¨‹

```bash
# 1. Pretrain (å°æ•°æ®é›†å¿«é€ŸéªŒè¯)
./run.sh pretrain-test

# æˆ–å®Œæ•´é¢„è®­ç»ƒ
./run.sh pretrain

# 2. SFT
./run.sh sft checkpoints/pretrain/best.pt

# 3. RL (GRPO)
./run.sh rl checkpoints/sft/best.pt

# 4. æ¨ç†
./run.sh inference checkpoints/rl/best.pt

# 5. äº¤äº’å¼å¯¹è¯
./run.sh chat checkpoints/rl/best.pt
```

### 8.4 ä¸€é”®å®Œæ•´æµç¨‹

```bash
# å¿«é€Ÿæµ‹è¯•æ•´ä¸ªæµç¨‹
./run.sh full-test

# å®Œæ•´è®­ç»ƒæµç¨‹
./run.sh full
```

---

## 9. é…ç½®è¯´æ˜

### 9.1 é…ç½®æ–‡ä»¶

| æ–‡ä»¶ | ç”¨é€” |
|------|------|
| `config_default.yaml` | é»˜è®¤é…ç½®ï¼ˆå°æ•°æ®é›†ï¼‰ |
| `config_large.yaml` | å¤§è§„æ¨¡è®­ç»ƒé…ç½® |

### 9.2 å‘½ä»¤è¡Œå‚æ•°

```bash
python train.py \
    --mode pretrain|sft|rl \     # è®­ç»ƒæ¨¡å¼
    --dataset_scale small|large \ # æ•°æ®é›†è§„æ¨¡
    --config config.yaml \        # é…ç½®æ–‡ä»¶
    --checkpoint path/to/ckpt \   # åŠ è½½æ£€æŸ¥ç‚¹
    --device auto|cuda|mps|cpu \  # è®¾å¤‡
    --test                        # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
```

### 9.3 å…³é”®é…ç½®é¡¹

```yaml
model:
  hidden_size: 512           # æ¨¡å‹ç»´åº¦
  num_hidden_layers: 6       # Transformer å±‚æ•°
  num_attention_heads: 8     # æ³¨æ„åŠ›å¤´æ•°
  
  moe:
    enabled: true
    num_experts: 16          # ä¸“å®¶æ•°é‡
    num_experts_per_tok: 2   # æ¯ token æ¿€æ´»ä¸“å®¶æ•°
    
  mtp:
    enabled: true
    num_predict_tokens: 2    # é¢å¤–é¢„æµ‹ token æ•°

pretraining:
  batch_size: 16
  learning_rate: 3e-4
  max_steps: 5000

sft:
  batch_size: 8
  learning_rate: 2e-5

rl:
  algorithm: "grpo"          # grpo, ppo, dpo
  group_size: 4
  kl_coef: 0.1
```

---

## 10. ç›‘æ§ä¸å¯è§†åŒ–

### 10.1 TensorBoard

```bash
# å¯åŠ¨ TensorBoard
./run.sh tensorboard

# æˆ–æ‰‹åŠ¨å¯åŠ¨
tensorboard --logdir=runs --port=6006
```

### 10.2 å¯è§†åŒ–å†…å®¹

| ç±»åˆ« | å†…å®¹ |
|------|------|
| **Loss** | train_loss, val_loss, perplexity |
| **Learning** | learning_rate, grad_norm |
| **Speed** | tokens/sec, samples/sec, steps/sec |
| **Attention** | æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾ |
| **MoE** | ä¸“å®¶ä½¿ç”¨åˆ†å¸ƒ, routing entropy |
| **Generation** | ç”Ÿæˆæ–‡æœ¬æ ·æœ¬ |
| **RL** | reward, kl_divergence, policy_loss |

### 10.3 è®­ç»ƒæ—¥å¿—ç¤ºä¾‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step:    100/5000 [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  2.0%             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Loss:   6.2345  (smoothed:   6.3012)                                 â”‚
â”‚ LR: 2.85e-04  Grad norm:   0.8721                                    â”‚
â”‚ Epoch: 1                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Speed:    12543 tok/s   98.2 samples/s   6.14 steps/s                â”‚
â”‚ Time:      16.3s  ETA:    13.1m                                      â”‚
â”‚ Tokens:      203,776                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## å‚è€ƒèµ„æ–™

- [DeepSeek-V2 Paper](https://arxiv.org/abs/2405.04434)
- [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
- [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347)
- [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)
